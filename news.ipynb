{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title   \n",
      "0  As U.S. budget fight looms, Republicans flip t...  \\\n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject   \n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews  \\\n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = r'/workspaces/group-b-news/True.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df1 = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title   \n",
      "0   Donald Trump Sends Out Embarrassing New Year’...  \\\n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject   \n",
      "0  Donald Trump just couldn t wish all Americans ...    News  \\\n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correctly formatted file path using raw string\n",
    "file_path = r'/workspaces/group-b-news/Fake.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df2 = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                headline  00  000  000m  000s   \n",
      "0      As U.S. budget fight looms, Republicans flip t...   0    0     0     0  \\\n",
      "1      U.S. military to accept transgender recruits o...   0    0     0     0   \n",
      "2      Senior U.S. Republican senator: 'Let Mr. Muell...   0    0     0     0   \n",
      "3      FBI Russia probe helped by Australian diplomat...   0    0     0     0   \n",
      "4      Trump wants Postal Service to charge 'much mor...   0    0     0     0   \n",
      "...                                                  ...  ..  ...   ...   ...   \n",
      "44893  McPain: John McCain Furious That Iran Treated ...   0    0     0     0   \n",
      "44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   0    0     0     0   \n",
      "44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   0    0     0     0   \n",
      "44896  How to Blow $700 Million: Al Jazeera America F...   0    0     0     0   \n",
      "44897  10 U.S. Navy Sailors Held by Iranian Military ...   0    0     0     0   \n",
      "\n",
      "       0045  0111  0112  0130  0149  ...  zor  zschaepe  zucker  zuckerberg   \n",
      "0         0     0     0     0     0  ...    0         0       0           0  \\\n",
      "1         0     0     0     0     0  ...    0         0       0           0   \n",
      "2         0     0     0     0     0  ...    0         0       0           0   \n",
      "3         0     0     0     0     0  ...    0         0       0           0   \n",
      "4         0     0     0     0     0  ...    0         0       0           0   \n",
      "...     ...   ...   ...   ...   ...  ...  ...       ...     ...         ...   \n",
      "44893     0     0     0     0     0  ...    0         0       0           0   \n",
      "44894     0     0     0     0     0  ...    0         0       0           0   \n",
      "44895     0     0     0     0     0  ...    0         0       0           0   \n",
      "44896     0     0     0     0     0  ...    0         0       0           0   \n",
      "44897     0     0     0     0     0  ...    0         0       0           0   \n",
      "\n",
      "       zulia  zuma  zummar  zurich  état  žižek  \n",
      "0          0     0       0       0     0      0  \n",
      "1          0     0       0       0     0      0  \n",
      "2          0     0       0       0     0      0  \n",
      "3          0     0       0       0     0      0  \n",
      "4          0     0       0       0     0      0  \n",
      "...      ...   ...     ...     ...   ...    ...  \n",
      "44893      0     0       0       0     0      0  \n",
      "44894      0     0       0       0     0      0  \n",
      "44895      0     0       0       0     0      0  \n",
      "44896      0     0       0       0     0      0  \n",
      "44897      0     0       0       0     0      0  \n",
      "\n",
      "[44898 rows x 20897 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# File paths\n",
    "file_path1 = r'/workspaces/group-b-news/True.csv'\n",
    "file_path2 = r'/workspaces/group-b-news/Fake.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Extract the 'title' column from each DataFrame\n",
    "headlines1 = df1['title'].tolist()\n",
    "headlines2 = df2['title'].tolist()\n",
    "\n",
    "# Combine the headlines from both DataFrames\n",
    "all_headlines = headlines1 + headlines2\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'headline': all_headlines})\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the headlines to get the one-hot encoding (sparse matrix)\n",
    "X = vectorizer.fit_transform(df['headline'])\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "one_hot_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Combine the one-hot encoded data with the original DataFrame\n",
    "df = pd.concat([df, one_hot_df], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 578ms/step\n",
      "Output from the model:\n",
      " [[0.49965474]\n",
      " [0.49965474]\n",
      " [0.49965474]\n",
      " ...\n",
      " [0.49965474]\n",
      " [0.49965474]\n",
      " [0.49965474]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# File paths\n",
    "file_path1 = r'/workspaces/group-b-news/True.csv'\n",
    "file_path2 = r'/workspaces/group-b-news/Fake.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Extract the 'title' column from each DataFrame\n",
    "headlines1 = df1['title'].tolist()\n",
    "headlines2 = df2['title'].tolist()\n",
    "\n",
    "# Combine the headlines from both DataFrames\n",
    "all_headlines = headlines1 + headlines2\n",
    "\n",
    "# Sample a subset of the data for testing\n",
    "sample_size = 5000  # Adjust as needed\n",
    "all_headlines = all_headlines[:sample_size]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'headline': all_headlines})\n",
    "\n",
    "# Initialize CountVectorizer with a limit on the number of features\n",
    "vectorizer = CountVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform the headlines to get the one-hot encoding (sparse matrix)\n",
    "X = vectorizer.fit_transform(df['headline'])\n",
    "\n",
    "# Convert sparse matrix to TensorFlow sparse tensor\n",
    "def to_sparse_tensor(sparse_matrix):\n",
    "    indices = np.array(list(zip(*sparse_matrix.nonzero())))\n",
    "    values = sparse_matrix.data\n",
    "    shape = sparse_matrix.shape\n",
    "    sparse_tensor = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=shape)\n",
    "    return tf.sparse.reorder(sparse_tensor)  # Reorder sparse tensor indices\n",
    "\n",
    "sparse_tensor = to_sparse_tensor(X)\n",
    "\n",
    "# Determine the vocabulary size and sequence length\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "sequence_length = X.shape[1]  # Use X.shape[1] for the number of features\n",
    "\n",
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "embedding_dim = 50  # This is the size of the dense vectors\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length))\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(100, return_sequences=False))  # 100 LSTM units\n",
    "\n",
    "# Add a Dense layer for output (binary classification example)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Example: Run the sparse tensor through the model\n",
    "# Convert sparse tensor to dense for prediction\n",
    "output = model.predict(tf.sparse.to_dense(sparse_tensor))\n",
    "print(\"Output from the model:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.8910 - loss: 0.2548 - val_accuracy: 0.9534 - val_loss: 0.1227\n",
      "Epoch 2/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9730 - loss: 0.0754 - val_accuracy: 0.9574 - val_loss: 0.1215\n",
      "Epoch 3/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9853 - loss: 0.0426 - val_accuracy: 0.9516 - val_loss: 0.1528\n",
      "Epoch 4/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9917 - loss: 0.0246 - val_accuracy: 0.9527 - val_loss: 0.1785\n",
      "Epoch 5/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9940 - loss: 0.0175 - val_accuracy: 0.9503 - val_loss: 0.1817\n",
      "Epoch 6/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9524 - val_loss: 0.2261\n",
      "Epoch 7/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9976 - loss: 0.0079 - val_accuracy: 0.9506 - val_loss: 0.2470\n",
      "Epoch 8/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9971 - loss: 0.0082 - val_accuracy: 0.9506 - val_loss: 0.2670\n",
      "Epoch 9/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9503 - val_loss: 0.2790\n",
      "Epoch 10/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 0.9497 - val_loss: 0.2942\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9486 - loss: 0.3000\n",
      "Test Loss: 0.2874627113342285, Test Accuracy: 0.9511135816574097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# File paths\n",
    "file_path1 = r'/workspaces/group-b-news/True.csv'\n",
    "file_path2 = r'/workspaces/group-b-news/Fake.csv'\n",
    "\n",
    "# Step 1: Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Extract the 'title' column from each DataFrame\n",
    "headlines1 = df1['title'].tolist()\n",
    "headlines2 = df2['title'].tolist()\n",
    "\n",
    "# Combine the headlines from both DataFrames\n",
    "headlines = headlines1 + headlines2\n",
    "\n",
    "# Create labels (1 for True, 0 for Fake) - Adjust as needed\n",
    "labels = [1] * len(headlines1) + [0] * len(headlines2)\n",
    "\n",
    "# Step 2: Tokenize the headlines\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "sequences = tokenizer.texts_to_sequences(headlines)\n",
    "\n",
    "# Step 3: Pad the sequences to ensure uniform length\n",
    "max_sequence_length = 10\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Step 4: Prepare labels\n",
    "y = np.array(labels)\n",
    "\n",
    "# Step 5: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Step 7: Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 8: Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# (Optional) Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.8994 - loss: 0.2481 - val_accuracy: 0.9516 - val_loss: 0.1280\n",
      "Epoch 2/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9738 - loss: 0.0733 - val_accuracy: 0.9552 - val_loss: 0.1286\n",
      "Epoch 3/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9873 - loss: 0.0393 - val_accuracy: 0.9542 - val_loss: 0.1576\n",
      "Epoch 4/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.0255 - val_accuracy: 0.9531 - val_loss: 0.1822\n",
      "Epoch 5/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9943 - loss: 0.0161 - val_accuracy: 0.9486 - val_loss: 0.2190\n",
      "Epoch 6/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9511 - val_loss: 0.2398\n",
      "Epoch 7/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0053 - val_accuracy: 0.9509 - val_loss: 0.2788\n",
      "Epoch 8/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 0.9509 - val_loss: 0.2711\n",
      "Epoch 9/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 0.9502 - val_loss: 0.3564\n",
      "Epoch 10/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0040 - val_accuracy: 0.9486 - val_loss: 0.3327\n",
      "\u001b[1m1404/1404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "                                                headline    prediction_text\n",
      "0      As U.S. budget fight looms, Republicans flip t...  This news is real\n",
      "1      U.S. military to accept transgender recruits o...  This news is real\n",
      "2      Senior U.S. Republican senator: 'Let Mr. Muell...  This news is real\n",
      "3      FBI Russia probe helped by Australian diplomat...  This news is real\n",
      "4      Trump wants Postal Service to charge 'much mor...  This news is real\n",
      "...                                                  ...                ...\n",
      "44893  McPain: John McCain Furious That Iran Treated ...  This news is fake\n",
      "44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...  This news is fake\n",
      "44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...  This news is fake\n",
      "44896  How to Blow $700 Million: Al Jazeera America F...  This news is fake\n",
      "44897  10 U.S. Navy Sailors Held by Iranian Military ...  This news is fake\n",
      "\n",
      "[44898 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# File paths\n",
    "file_path1 = r'/workspaces/group-b-news/True.csv'\n",
    "file_path2 = r'/workspaces/group-b-news/Fake.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Extract the 'title' column from each DataFrame\n",
    "headlines1 = df1['title'].tolist()\n",
    "headlines2 = df2['title'].tolist()\n",
    "\n",
    "# Combine the headlines from both DataFrames\n",
    "headlines = headlines1 + headlines2\n",
    "\n",
    "# Create labels (1 for True, 0 for Fake)\n",
    "labels = [1] * len(headlines1) + [0] * len(headlines2)\n",
    "\n",
    "# Tokenize the headlines\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "sequences = tokenizer.texts_to_sequences(headlines)\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "max_sequence_length = 10\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Prepare labels\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict on the entire dataset\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_df = pd.DataFrame({\n",
    "    'headline': headlines,\n",
    "    'actual_label': labels,\n",
    "    'predicted_label': predicted_labels,\n",
    "    'prediction_text': ['This news is real' if label == 1 else 'This news is fake' for label in predicted_labels]\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(results_df[['headline', 'prediction_text']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
